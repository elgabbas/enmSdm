% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/trainBrt.r
\name{trainBrt}
\alias{trainBrt}
\title{Calibrate a boosted regresion tree (generalied boosting machine) model}
\usage{
trainBrt(data, resp = names(data)[1],
  preds = names(data)[2:ncol(data)], family = "bernoulli",
  learningRate = c(1e-04, 0.001, 0.01), treeComplexity = c(9, 3, 1),
  bagFraction = 0.6, maxTrees = 4000, tries = 5,
  tryBy = c("learningRate", "treeComplexity", "maxTrees", "stepSize"),
  w = TRUE, out = "model", verbose = FALSE, ...)
}
\arguments{
\item{data}{data frame with first column being response}

\item{resp}{Character or integer. Name or column index of response variable. Default is to use the first column in \code{data}.}

\item{preds}{Character list or integer list. Names of columns or column indices of predictors. Default is to use the second and subsequent columns in \code{data}.}

\item{family}{Character. Name of error family.  See [dismo::gbm.step()].}

\item{learningRate}{Numeric. Learning rate at which model learns from successive trees (recommended range 0.0001 to 0.1).}

\item{treeComplexity}{Positive integer. Tree complexity: depth of branches in a single tree (2 to 16).}

\item{bagFraction}{Numeric in the range (0, 1). Bag fraction: proportion of data used for training in cross-validation (recommended range recommended 0.5 to 0.7).}

\item{maxTrees}{Positive integer. Maximum number of trees in model set (same as parameter \code{max.trees} in [dismo::gbm.step()]).}

\item{tries}{Integer > 0. Number of times to try to train a model with a particular set of tuning parameters. The function will stop training the first time a model converges (usually on the first attempt). Non-convergence seems to be related to the number of trees tried in each step.  So if non-convergence occurs then the function automatically increases the number of trees in the step size until \code{tries} is reached.}

\item{tryBy}{Character list. A list that contains one or more of \code{'learningRate'}, \code{'treeComplexity'}, \code{numTrees}, and/or \code{'stepSize'}. If a given combination of \code{learningRate}, \code{treeComplexity}, \code{numTrees}, \code{stepSize}, and \code{bagFraction} do not allow model convergence then then the function tries again but with alterations to any of the arguments named in \code{tryBy}:
* \code{learningRate}: Decrease the learning rate by a factor of 10.
* \code{treeComplexity}: Randomly increase/decrease tree complexity by 1 (minimum of 1).
* \code{maxTrees}: Increase number of trees by 20%.
* \code{stepSize}: Increase step size (argument \code{n.trees} in \code{gbm.step()}) by 50%.
If \code{tryBy} is NULL then the function attempts to train the model with the same parameters up to \code{tries} times.}

\item{w}{Either logical in which case TRUE causes the total weight of presences to equal the total weight of absences (if \code{family='binomial'}) OR a numeric list of weights, one per row in \code{data} OR the name of the column in \code{data} that contains site weights. The default is to assign a weight of 1 to each datum.}

\item{out}{Character. Indicates type of value returned. If \code{model} (default) then returns an object of class \code{gbm}. If \code{tuning} then just return a data frame with tuning parameters and deviance of each model sorted by deviance. If both then return a 2-item list with the best model and the tuning table.}

\item{verbose}{Logical. If TRUE display progress.}

\item{...}{Arguments to pass to `gbm.step`.}
}
\value{
If \code{out = 'model'} this function returns an object of class \code{gbm}. If \code{out = 'tuning'} this function returns a data frame with tuning parameters and cross-validation deviance for each model tried. If \code{out = c('model', 'tuning'} then it returns a list object with the \code{gbm} object and the data frame.
}
\description{
This function is a wrapper for \code{gbm.step()}. It returns the model with best combination of learning rate, tree depth, and bag fraction based on cross-validated deviance. Models with >=1000 trees are preferred over those with lower deviance.
}
\examples{
\donttest{
set.seed(123)
x <- matrix(rnorm(n = 6*100), ncol = 6)
# true variables will be #1, #2, #5, and #6, plus
# the squares of #1 and #6, plus
# interaction between #1 and #6
# the cube of #5
imp <- c('x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x1_pow2', 'x6_pow2', 'x1_by_x6', 'x5_pow3')
betas <- c(5, 2, 0, 0, 1, -1, 8, 1, 2, -4)
names(betas) <- imp
y <- 0.5 + x \%*\% betas[1:6] + betas[7] * x[ , 1] +
betas[8] * x[ , 6] + betas[9] * x[ , 1] * x[ , 6] + betas[10] * x[ , 5]^3
y <- as.integer(y > 10)
x <- cbind(y, x)
x <- as.data.frame(x)
names(x) <- c('y', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6')
out <- trainBrt(
    x,
    treeComplexity=c(1, 3),
    learningRate=c(0.01, 0.001),
    bagFraction=0.6,
    maxTrees=1000,
    out=c('tuning', 'model'),
    verbose=TRUE
)
plot(out$model)
out$tuning
}
}
\seealso{
[dismo::gbm.step()]
}
